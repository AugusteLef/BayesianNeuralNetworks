BayesianLayer:
__init__
Using 'Practical Variational Inference for Neural Network' paper, and especially the table 1, we setted priors (mu, sigma) to 0 and 0.1 and the weights(mu, log_sigma) to 0 and log(0.075).
forward()
Based on the Blitz library and other ressources we sample the weight and the bias and return the produdct of inputs with weitghts and add the bias. 
_kl_divergence()
Using given values and self.priors values we create two LogNormal distribution, p and q in order to compute the kl_divergence / distance between these two distribution. In order to compute the KL Divergence we used the method given by torch (torch.distributions.kl_divergence(p, q)). As this function return a distribution of the divergence, we take the mean in order to obtaine a 'loss'.
BayesNet:
predict_class_probs()
We computed the categorical softmax probabilities by summing num_forward_passes times the probs given by the softmax function. Then we marginalize the results (we divide it by num_forward_passes). We return the marginalized result.
kl_loss()
Here we iterate over all layers and sum the loss for each of them (given by kl_divergence). Finally we take the 'average' loss given by dividing the sum of loss by the number of iteration (eq. layers) as the model loss. We return the model loss. 
Training:
train_network():
Here we simply add the kl_loss to the loss of the model during the training process.
Parameters:
Epoch: we saw that our results converges fastly enough after 10 epoch. When we tried to perform more of them (25, 50, etc) we overfitt the training set and then failed to do correct predicitons for the testing set. 
BatchSize: According to the experiment of Yann Lecun, 256 is the optimal batchsize for MNIST data set. So we decided to set the batchSize to 256. We also tried other values (32, 64, 128) but we obtained lower accuracy and bigger loss. 
Learning rate: As we used the adam optimizer we decided to set the learning rate to 1e-3 as it is the default value for this optimizer. It works well enought so we did not tried other values.


